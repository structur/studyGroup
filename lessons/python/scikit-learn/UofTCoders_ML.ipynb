{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "\"Machine learning gives computers the ability to learn without explicitly programmed.\" - Arthur Samuel, 1959\n",
    "\n",
    "Machine learning originated from pattern recognition and computational learning theory in AI. It is the study and construction of algorithms to learn from and make predictions on data through building a model from sample input.\n",
    "\n",
    "### Some uses of learning algorithms\n",
    "1. Classification: determine which discrete category the example is\n",
    "2. Recognizing patterns: speech recognition, facial identity ...\n",
    "3. Recommender Systems: noisy data, commercial pay-off (e.g., Amazon, Netflix)\n",
    "4. Information retrieval: find documents or images with similar content\n",
    "5. Computer vision: detection, segmentation, depth estimation, optical flow ...\n",
    "6. Robotics: perception, planning ...\n",
    "7. Learning to play games: AlphaGO\n",
    "8. Recognizing anomalies: Unusual sequences of credit card transactions, panic situation at an airport\n",
    "9. Spam filtering, fraud detection: The enemy adapts so we must adapt too\n",
    "\n",
    "### Types of learning tasks\n",
    "- Supervised Learning: correct output known for each training example for predicting output when given an input vector\n",
    "\n",
    "    1. Classification: 1-of-N output, e.g. object recognition, medical diagnosis\n",
    "    2. Regression: real-valued-output, e.g.predicting market prices, customer ratings\n",
    "\n",
    "\n",
    "- Unsupervised Learning: for learning an internal representation of the input to capture regularities and structure in the data without any labels\n",
    "\n",
    "    1. Clustering: dividing input into groups that are unknown beforehand\n",
    "    2. Dimensionality reduction: extract informative features, e.g. PCA, t-SNE\n",
    "\n",
    "\n",
    "- Reinforcement Learning: perform an action with the goal to maximize payoff by the feedback of reward and punishments, e.g. playing a game against an opponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this tutorial\n",
    "\n",
    "1. Regression (e.g. Linear Regression)\n",
    "2. Classification (e.g. SVM, Logistic regression)\n",
    "3. Clustering (e.g. K-means)\n",
    "4. Dimensionality Reduction (e.g. PCA)\n",
    "5. Sklearn's TPOT package for optimized machine learning pipelines\n",
    "\n",
    "### We won't cover:\n",
    "1. Other classification and clustering algorithms (e.g. Neural Networks, Hierarchical Clustering)\n",
    "2. Model selection (such as Bayesian Information Criteria - BIC)\n",
    "3. Hyper-parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful websites\n",
    "Sklearn class and function reference page\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "\n",
    "Machine Learning course from Computer Science Department, UOFT\n",
    "http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/CSC411_Fall16.html\n",
    "\n",
    "TPOT for optimized machine learning pipelines\n",
    "https://rhiever.github.io/tpot/\n",
    "\n",
    "KERAS package for Neural Networks\n",
    "https://keras.io/\n",
    "\n",
    "Tensorflow Playground for Deep Neural Networks\n",
    "http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.69754&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "### Samples of 3 different species of Iris flower with 4 features (sepal and petal lengths and widths)\n",
    "### We will learn:\n",
    "- Displaying feature relationships, correlations\n",
    "- Dimensionality reduction using PCA\n",
    "- Scaling the data\n",
    "- Classification and clustering algorithms\n",
    "- Compare to another image dataset of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data = np.c_[iris['data'], iris['target']],\n",
    "                       columns = iris['feature_names'] + ['target'])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Correlations\n",
    "iris_df.iloc[:,:4].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Relationships\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "key = {0: ('red', 'Iris setosa'),\n",
    "       1: ('blue', 'Iris versicolor'),\n",
    "       2: ('green', 'Iris virginica')}\n",
    "colors = [key[index][0] for index in Y]\n",
    "# Plot the training points\n",
    "\n",
    "# Sepal information\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Sepal Information')\n",
    "\n",
    "# Petal information\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 2], X[:, 3], c=colors)\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Petal Information')\n",
    "\n",
    "# Length information\n",
    "plt.subplot(223)\n",
    "plt.scatter(X[:, 0], X[:, 2], c=colors)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal length')\n",
    "plt.title('Length Information')\n",
    "\n",
    "# Width information\n",
    "plt.subplot(224)\n",
    "plt.scatter(X[:, 1], X[:, 3], c=colors)\n",
    "plt.xlabel('Sepal width')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Width Information')\n",
    "\n",
    "# Plot legend\n",
    "patches = [matplotlib.patches.Patch(color=color, label=label) for color, label in key.values()]\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.1,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA to iris features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "iris_pca = pca.fit_transform(iris.data)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(iris_pca[:, 0], iris_pca[:, 1], c=colors)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA')\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.2,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting features from PCA\n",
    "from scipy import stats\n",
    "\n",
    "pca_corr = pd.DataFrame(columns = iris['feature_names'])\n",
    "for i in range(2):\n",
    "    print('PC%d explained variance = %.2f%s\\n' % (i+1, pca.explained_variance_ratio_[i]*100, '%'))\n",
    "    corr = []\n",
    "    for j in range(4):\n",
    "        corr.append(stats.pearsonr(iris_pca[:, i], iris.data[:, j])[0])\n",
    "    pca_corr.loc[i, ] = corr\n",
    "pca_corr = pca_corr.rename(index={0: 'PC1', 1: 'PC2'})\n",
    "pca_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# When the data is scaled to complete data\n",
    "X_scaled = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "# When the data is scaled to a part of the data\n",
    "#scaler = preprocessing.StandardScaler().fit(X)\n",
    "#X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Feature Relationships\n",
    "# Plot the training points\n",
    "\n",
    "# Sepal information\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=colors)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Sepal Information')\n",
    "\n",
    "# Petal information\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_scaled[:, 2], X_scaled[:, 3], c=colors)\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Petal Information')\n",
    "\n",
    "# Length information\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 2], c=colors)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal length')\n",
    "plt.title('Length Information')\n",
    "\n",
    "# Width information\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_scaled[:, 1], X_scaled[:, 3], c=colors)\n",
    "plt.xlabel('Sepal width')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Width Information')\n",
    "\n",
    "# Plot legend\n",
    "patches = [matplotlib.patches.Patch(color=color, label=label) for color, label in key.values()]\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.1,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA to iris features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "iris_pca_scaled = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(iris_pca_scaled[:, 0], iris_pca_scaled[:, 1], c=colors)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-3,3])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA with Scaled Data')\n",
    "\n",
    "# PCA on original data from the previous cell\n",
    "pca = PCA(n_components=2)\n",
    "iris_pca = pca.fit_transform(iris.data)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(iris_pca[:, 0], iris_pca[:, 1], c=colors)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-3,3])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA')\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.1,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting features from PCA\n",
    "from scipy import stats\n",
    "\n",
    "pca_corr = pd.DataFrame(columns = iris['feature_names'])\n",
    "for i in range(2):\n",
    "    print('PC%d explained variance = %.2f%s\\n' % (i+1, pca.explained_variance_ratio_[i]*100, '%'))\n",
    "    corr = []\n",
    "    for j in range(4):\n",
    "        corr.append(stats.pearsonr(iris_pca_scaled[:, i], X_scaled[:, j])[0])\n",
    "    pca_corr.loc[i, ] = corr\n",
    "pca_corr = pca_corr.rename(index={0: 'PC1', 1: 'PC2'})\n",
    "pca_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris with 2 classes\n",
    "iris_df2 = iris_df[(iris_df.target == 0) | (iris_df.target == 1)]\n",
    "\n",
    "# Plot 2-Class Iris data\n",
    "iris2 = np.asarray(iris_df2.iloc[:, :4])\n",
    "Y2 = list(iris_df2.target)\n",
    "key = {0: ('red', 'Iris setosa'),\n",
    "       1: ('blue', 'Iris versicolor')}\n",
    "colors = [key[index][0] for index in Y2]\n",
    "# Plot the training points\n",
    "\n",
    "# Sepal information\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(iris2[:, 0], iris2[:, 1], c=colors)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Sepal Information')\n",
    "# Width information\n",
    "plt.subplot(122)\n",
    "plt.scatter(iris2[:, 2], iris2[:, 3], c=colors)\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.title('Petal Information')\n",
    "\n",
    "# Plot legend\n",
    "patches = [matplotlib.patches.Patch(color=color, label=label) for color, label in key.values()]\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.1,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification - Logistic Regression and Linear Regression as a means of Classification\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Model fit\n",
    "lr = linear_model.LogisticRegression()\n",
    "#lr = linear_model.LinearRegression()\n",
    "lr.fit(iris2, Y2)\n",
    "\n",
    "# Predict values\n",
    "Y_pred = lr.predict(iris2)\n",
    "\n",
    "# Calculate performance - accuracy\n",
    "lr.score(iris2, Y2)\n",
    "\n",
    "# Using some features\n",
    "#lr.fit(iris2[:,:2], Y2)\n",
    "#Y_pred = lr.predict(iris2[:,:2])\n",
    "#lr.score(iris2[:,:2], Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification - Logistic Regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Load data to classify: Complete, partial, scaled, PCA applied\n",
    "X = iris.data\n",
    "#X = iris.data[:,:2]\n",
    "#X = X_scaled\n",
    "#X = iris_pca\n",
    "Y = iris.target\n",
    "\n",
    "# Model fit\n",
    "lr_multi = linear_model.LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "lr_multi.fit(X, Y)\n",
    "\n",
    "# Predict values\n",
    "Y_pred = lr_multi.predict(X)\n",
    "\n",
    "# Calculate performance - accuracy\n",
    "lr_multi.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split labeled data into training and test set\n",
    "X = iris.data\n",
    "#X = iris.data[:,:2]\n",
    "#X = X_scaled\n",
    "#X = iris_pca\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, iris.target, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Training set size')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print('Test set size')\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Model fit\n",
    "lr_multi = linear_model.LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "lr_multi.fit(X_train, Y_train)\n",
    "\n",
    "# Predict values\n",
    "Y_pred = lr_multi.predict(X_test)\n",
    "\n",
    "# Calculate performance - accuracy\n",
    "lr_multi.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Enter values\n",
    "model = lr_multi\n",
    "data = X_test\n",
    "actual_target = Y_test\n",
    "predicted_target = Y_pred\n",
    "classes = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "# Calculates confusion matrix from actual and predicted target values\n",
    "cnf = confusion_matrix(actual_target, predicted_target)\n",
    "# Accuracy results\n",
    "acc = model.score(data, actual_target)*100\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, acc):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: Acc %.2f%%' % acc)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment='center',color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm=cnf, classes=classes, acc=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification using Support Vector Machines\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Split labeled data into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Model fit - SVM with linear kernel\n",
    "svm_ = svm.SVC(kernel='linear')\n",
    "# Model fit - SVM with non-linear kernel (RBF) for data not linearly separable\n",
    "# svm = svm.SVC(kernel='rbf')\n",
    "svm_.fit(X_train, Y_train)\n",
    "\n",
    "# Predict values\n",
    "Y_pred = svm_.predict(X_test)\n",
    "\n",
    "# Calculate performance - accuracy\n",
    "print('1-fold CV accuracy: %.2f' % svm_.score(X_test, Y_test))\n",
    "\n",
    "scores = cross_val_score(svm_, iris.data, iris.target, cv=5)\n",
    "print('\\n5-fold CV accuracies:')\n",
    "print(scores)\n",
    "print('\\n5-fold CV average accuracy: %.2f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with image dataset of handwritten digits\n",
    "digits = datasets.load_digits()\n",
    "X_d = digits.data\n",
    "Y_d = digits.target\n",
    "print('Digits image shape: %d x %d pixels'  % (digits.images.shape[1], digits.images.shape[2]))\n",
    "print('Digits data shape: %d x %d'  % (digits.data.shape[0], digits.data.shape[1]))\n",
    "print('Digit targets: ', set(Y_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fit - using cross-validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target, test_size=0.4, random_state=0)\n",
    "lr_multi_d = linear_model.LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "# SVM produces higher accuracy (try kernel = 'rbf')\n",
    "#lr_multi_d = svm.SVC(kernel='linear')\n",
    "lr_multi_d.fit(X_train, Y_train)\n",
    "\n",
    "# Predict values\n",
    "Y_pred = lr_multi_d.predict(X_test)\n",
    "\n",
    "# Enter values for confusion matrix\n",
    "model = lr_multi_d\n",
    "data = X_test\n",
    "actual_target = Y_test\n",
    "predicted_target = Y_pred\n",
    "classes = digits.target_names\n",
    "\n",
    "# Calculates confusion matrix from actual and predicted target values\n",
    "cnf = confusion_matrix(actual_target, predicted_target)\n",
    "# Accuracy results\n",
    "acc = model.score(data, actual_target)*100\n",
    "\n",
    "plot_confusion_matrix(cm=cnf, classes=classes, acc=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Initialization is important in k-means\n",
    "# k-means++ selects initial cluster centers for k-mean clustering in a smart way to speed up convergence\n",
    "km = KMeans(n_clusters = 3, init='k-means++')\n",
    "# Random initialization performs worse\n",
    "#km = KMeans(n_clusters = 3, init='random')\n",
    "# Cluster centers learned from a previous successful run\n",
    "#km = KMeans(n_clusters = 3, init=cc)\n",
    "\n",
    "Y_pred = km.fit_predict(X)\n",
    "cc = km.cluster_centers_\n",
    "\n",
    "# Visualize datapoints with targets and cluster centers\n",
    "# Actual targets\n",
    "key = {0: ('red', 'Iris setosa'),\n",
    "       1: ('blue', 'Iris versicolor'),\n",
    "       2: ('green', 'Iris virginica')}\n",
    "colors_target = [key[index][0] for index in Y]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors_target)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Targets')\n",
    "patches = [matplotlib.patches.Patch(color=color, label=label) for color, label in key.values()]\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.1,-0.1,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "\n",
    "# Cluster Results\n",
    "key = {0: ('pink', 'Cluster-1'),\n",
    "       1: ('orange', 'Cluster-2'),\n",
    "       2: ('purple', 'Cluster-3')}\n",
    "colors_cluster = [key[index][0] for index in Y_pred]\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors_cluster)\n",
    "plt.scatter(cc[:, 0], cc[:, 1], c='black', marker='x', s=60) # Plot cluster centers\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Clusters')\n",
    "\n",
    "# Plot legend\n",
    "patches = [matplotlib.patches.Patch(color=color, label=label) for color, label in key.values()]\n",
    "plt.legend(handles=patches, labels=[label for _, label in key.values()],\n",
    "           bbox_to_anchor = (0.075,-0.3,1,1), bbox_transform = plt.gcf().transFigure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter values for confusion matrix\n",
    "model = km\n",
    "data = X\n",
    "actual_target = Y\n",
    "predicted_target = Y_pred\n",
    "classes = iris.target_names\n",
    "\n",
    "# Plot cluster results and labels\n",
    "cnf = confusion_matrix(actual_target, predicted_target)\n",
    "plt.imshow(cnf, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Clusters with corresponding targets')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, np.array(list(set(Y_pred))))\n",
    "plt.yticks(tick_marks, classes)\n",
    "thresh = cnf.max() / 2.\n",
    "for i, j in itertools.product(range(cnf.shape[0]), range(cnf.shape[1])):\n",
    "    plt.text(j, i, cnf[i, j], horizontalalignment='center',color='white' if cnf[i, j] > thresh else 'black')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Cluster assignment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston housing prices dataset\n",
    "### 506 samples with 13 features with continuous values\n",
    "### We will learn:\n",
    "    - Regression\n",
    "    - Error as a mean of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "boston_df = pd.DataFrame(data = np.c_[boston['data'], boston['target']],\n",
    "                         columns = list(boston['feature_names']) + ['target'])\n",
    "\n",
    "print('Boston Housing Prices: dataframe shape')\n",
    "print(boston_df.shape)\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between features and target value\n",
    "from scipy import stats\n",
    "\n",
    "X = boston_df\n",
    "Y = boston.target\n",
    "features = boston['feature_names']\n",
    "\n",
    "# Plot correlation for each feature\n",
    "i=1\n",
    "plt.figure(figsize=(20, 20))\n",
    "for f in features:\n",
    "    corr = stats.pearsonr(list(X[f]), Y)[0]\n",
    "    plt.subplot(4, 4, i)\n",
    "    plt.scatter(list(X[f]), Y)\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel('Target Housing Price')\n",
    "    plt.title(('%s and target correlation = %.2f') % (f, corr))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Data for regression\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "#X = pca.fit_transform(X)\n",
    "# 2 features with highest correlation\n",
    "#X = np.c_[list(boston_df.RM), list(boston_df.LSTAT)]\n",
    "\n",
    "# Model - Linear, Lasso, Ridge, normalized or not\n",
    "#lr = linear_model.LinearRegression(normalize=False)\n",
    "lr = linear_model.Lasso(normalize=False)\n",
    "#lr = linear_model.Ridge(normalize=False)\n",
    "\n",
    "# Model fit\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "lr.fit(X_train, Y_train)\n",
    "# Score: Correlation between actual and predicted values\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Percentage of Variance Explained with additional Principal Components\n",
    "exp_var = []\n",
    "for i in range(X.shape[1]):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X)\n",
    "    summ = sum(pca.explained_variance_ratio_)\n",
    "    exp_var.append(summ)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(exp_var)\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Total % of variance explained')\n",
    "plt.title('PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT\n",
    "#### for optimized machine learning pipelines\n",
    "#### https://rhiever.github.io/tpot/\n",
    "#### Examples: https://rhiever.github.io/tpot/examples/\n",
    "#### TPOT is a Python tool that automatically creates and optimizes machine learning pipelines using genetic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](tpot-ml-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](tpot-pipeline-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn import datasets\n",
    "# pip install tpot\n",
    "\n",
    "data = datasets.load_iris()\n",
    "#data = datasets.load_digits()\n",
    "export_file = 'tpot_pipeline_iris.py'\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export(export_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_boston()\n",
    "export_file = 'tpot_pipeline_boston.py'\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export(export_file)\n",
    "\n",
    "y_pred = tpot.predict(X_test)\n",
    "print('\\nCorrelation between predicted and actual target values: %.2f' % stats.pearsonr(y_pred, y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
